{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data partitioning\n",
    "\n",
    "In order to effectively use the available data, it's important to separate it in sets meant for training the models, and sets meant for evaluation of the data, in order to avoid overfitting the model to the available data.<br>\n",
    "There are plenty of strategies one can use for splitting the data, all with their respective benefits and downsides\n",
    "\n",
    "But first, let's get the libraries and data to be used throughout this entire notebook\n",
    "\n",
    "<b>1. Import the needed libraries (pandas as pd and numpy as np)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Read the dataset `diabetes.csv`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('datasets/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Create an object named `data` that will contain your independent features and another object named `target` that will contain your independent feature/target (last column in the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = diabetes.iloc[:,:-1]\n",
    "target = diabetes.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the material in this notebook is only applicable to exclusively numerical data. For cases like these, it's important to properly filter features, separating the numeric features from the non-numeric features. This can be done with the pandas method `.select_dtypes()`, that can include and/or exclude features of a given data type. Numeric variables are identifiable by their data type, thanks to the `numpy` library\n",
    "\n",
    "- _Documentation pandas.DataFrame.select_dtypes():_ https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html\n",
    "    \n",
    "<b>4. Define a new object named `data_n` where only the numerical variables are mantained, setting the `include` parameter's value to `np.number`, and a object named as `data_c` with all the categorical independent variables. These objects will be used later in the notebook.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = data.select_dtypes(include=np.number).set_index(data.index)\n",
    "data_c = data.select_dtypes(exclude=np.number).set_index(data.index)#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "data_c.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go into the data.\n",
    "\n",
    "### 1.1. The train-test split\n",
    "\n",
    "<img src=\"./img/training-validation-test-data-set.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "This is the simplest and most common approach: splitting the data into two sets, one for training the model (`train`) and one for model validation purposes (`test`). Ideally the data is split leaving 70-80% of observations for training, and the rest for validation.\n",
    "\n",
    "In this exercise, we are going to split our dataset into train, test and validation. <br>\n",
    "By default, sklearn has a function named `train_test_split`, that was used in the last week, that allows to split the dataset into two different datasets.\n",
    "\n",
    "- _Documentation: sklearn.model_selection.train_test_split():_ https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "<b>5. Import the function `train_test_split` from `sklearn.model_selection`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Divide the `data`into `X_train_val` and `X_test`, the `target`into `y_train_val` and `y_test`, and define the following arguments: `test_size = 0.2`, `random_state = 15`, `shuffle = True` and `stratify = target` </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 15,\n",
    "                                                    shuffle = True,\n",
    "                                                    stratify = target\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create two different datasets, one for train (80% of the data) and one for test (20% of the data). <br>\n",
    "`shuffle` randomizes the order of the observations, and `stratify` makes it so that every dataset resulting from the split has the same proportion of each label of the dependent variable.\n",
    "\n",
    "\n",
    "### How to create the three datasets: train, validation and test?\n",
    "To create three datasets (train, validation and test) with the function train_test_split, the function has to be called twice. <br>\n",
    "First we are going to create two sets of datasets, one for test (X_test and y_test) and another one that includes the data for training and validation (X_train_val and y_train_val).\n",
    "\n",
    "<b>7. Divide the `X_train_val`into `X_train` and `X_val`, the `y_train_val` into `y_train` and `y_val`, and define the following arguments: `test_size = 0.25`, `random_state = 15`, `shuffle = True` and `stratify = y_train_val`.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, \n",
    "                                                    y_train_val, \n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 15,\n",
    "                                                    shuffle = True,\n",
    "                                                    stratify = y_train_val\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>8. Run the cell below to check the proportion of data for each dataset. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:0.6% | validation:0.2% | test:0.2%\n"
     ]
    }
   ],
   "source": [
    "print('train:{}% | validation:{}% | test:{}%'.format(round(len(y_train)/len(target),2),\n",
    "                                                     round(len(y_val)/len(target),2),\n",
    "                                                     round(len(y_test)/len(target),2)\n",
    "                                                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three different datasets, namely:\n",
    "- Training dataset, with 60% of the data, that will allow me to build the model;\n",
    "- Validation dataset, with 20% of the data, that will allow me to fine tune the model and check some problems like overfitting;\n",
    "- Test dataset, with 20% of the data, that will allow me to evaluate the performance of the final model.\n",
    "\n",
    "With this approach, there is a possibility of high bias if we have limited data, since there's a higher chance of missing important information for training. Additionally, the model's performance on validation data is a less reliable indicator of the model's performance on new data, since the amount of validation data is rather small.<br>\n",
    "If there's a high amount of data, and the test sample has the same distribution as the train sample, then this approach is acceptable.\n",
    "\n",
    "****\n",
    "\n",
    "The different techniques we are going to check in the next steps are commonly used in applied machine learning to compare and select a model for a given predictive modeling problem, since they . <br>\n",
    "In the following cases, we are going to check the performance of a Logistic Regression using those different techniques.\n",
    "\n",
    "### 1.2. K-Fold Cross-Validation\n",
    "\n",
    "<img src=\"./img/kfold.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "This approach's idea is to make the most of the available data, whilst avoiding overlap between validation datasets.<br>\n",
    "There are multiple steps to this approach:\n",
    "1. Split the data into k partitions of equal size\n",
    "2. Each partition is used for testing the model once. When a partition is used for testing, all others are used for training\n",
    "3. The subsets are stratified before validation\n",
    "4. The estimations from each test are averaged, resulting in an overall estimate\n",
    "\n",
    "In the following examples we are going to only use the numeric variables of our dataset\n",
    "\n",
    "<b>9. Check `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>10. Import `KFold` from `sklearn.model_selection`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>11. Import `LogisticRegression` from `sklearn.linear_model`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>12. Create a function named as `run_model_LR` that receives as parameters the dependent variable and the independent variables and returns a fitted Logistic Regression model to the data. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_model_LR(X,y):\n",
    "    logge = LogisticRegression()\n",
    "    logge.fit(X,y)\n",
    "    return logge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>13. Create a function named as `evaluate_model` that receives as parameters the independent variables, the dependent variable and the model and returns the ``score`` method result.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y,model):\n",
    "    return model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>14. Run the cell below to create a function named `avg_score_LR` that will return the average score value for the train and the test set. This will have as parameters the partition technique you are going to use, your dependent variable and your independent variables.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score_LR(method,X,y):\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "\n",
    "    for train_index, test_index in method.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model = run_model_LR(X_train, y_train)\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_test = evaluate_model(X_test,y_test, model)\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Test:', np.mean(score_test))\n",
    "    \n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>15. Create a KFold Instance where the number of splits is 10 (*n_splits*) and name it as `kf`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>16. Call the function `avg_score_LR` and check the average score for the train and the test sets using `kf`</b>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7838532996494985\n",
      "Test: 0.7721291866028708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7988422575976846,\n",
       "  0.7756874095513748,\n",
       "  0.7858176555716353,\n",
       "  0.788712011577424,\n",
       "  0.7829232995658466,\n",
       "  0.7901591895803184,\n",
       "  0.768451519536903,\n",
       "  0.7742402315484804,\n",
       "  0.7933526011560693,\n",
       "  0.7803468208092486],\n",
       " [0.7012987012987013,\n",
       "  0.8441558441558441,\n",
       "  0.7402597402597403,\n",
       "  0.6883116883116883,\n",
       "  0.7922077922077922,\n",
       "  0.7402597402597403,\n",
       "  0.8571428571428571,\n",
       "  0.8181818181818182,\n",
       "  0.7368421052631579,\n",
       "  0.8026315789473685])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score_LR(kf, data,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Repeated K-Fold\n",
    "\n",
    "Repeated K-Fold is, as the name says, running K-Fold multiple times, and averaging the results of each time K-Fold is ran\n",
    "\n",
    "<b>17. Import `RepeatedKFold` from `sklearn.model_selection`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>18. Create a RepeatedKFold Instance where the number of splits is 6 (`n_splits=6`) and the number of times cross-validator needs to be repeated is 2 (`n_repeats=2`)  and name it as `rkf`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits = 6, n_repeats = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>19. Call the function `avg_score_LR` and check the average score for the train and the test sets using `rkf`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7828125\n",
      "Test: 0.771484375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.784375,\n",
       "  0.78125,\n",
       "  0.7875,\n",
       "  0.7703125,\n",
       "  0.790625,\n",
       "  0.7796875,\n",
       "  0.78125,\n",
       "  0.79375,\n",
       "  0.7828125,\n",
       "  0.7796875,\n",
       "  0.7890625,\n",
       "  0.7734375],\n",
       " [0.734375,\n",
       "  0.7578125,\n",
       "  0.75,\n",
       "  0.8515625,\n",
       "  0.7265625,\n",
       "  0.78125,\n",
       "  0.78125,\n",
       "  0.75,\n",
       "  0.78125,\n",
       "  0.8046875,\n",
       "  0.765625,\n",
       "  0.7734375])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score_LR(rkf,data,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Leave One Out\n",
    "\n",
    "The Leave One Out method is the most extreme version of K-Fold Cross Validation: for _n_ observations. the Leave One Out method will have _n_ splits, and _n_ training phases. <br>\n",
    "For each time data is trained, a prediction is made for the one obsevation left out. <br>\n",
    "This method is very effective at making sure that there's no overlap, and that our data is reliable. However, it's very computationally expensive, and will take a lot of time to run for higher values of N.\n",
    "\n",
    "<b>20. Do the same steps you applied on the previous techniques, but this time using the Leave One Out. For that, you need to import `LeaveOneOut` from `sklearn.model_selection`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7817372202303345\n",
      "Test: 0.7799479166666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7770534550195567,\n",
       "  0.7796610169491526,\n",
       "  0.7757496740547588,\n",
       "  0.7770534550195567,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7744458930899609,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7848761408083442,\n",
       "  0.788787483702738,\n",
       "  0.7783572359843546,\n",
       "  0.7770534550195567,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.78748370273794,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.78748370273794,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7770534550195567,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7770534550195567,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7757496740547588,\n",
       "  0.7796610169491526,\n",
       "  0.7757496740547588,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.788787483702738,\n",
       "  0.7861799217731421,\n",
       "  0.7796610169491526,\n",
       "  0.7757496740547588,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7770534550195567,\n",
       "  0.7770534550195567,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7770534550195567,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7757496740547588,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7731421121251629,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7770534550195567,\n",
       "  0.7861799217731421,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7848761408083442,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7757496740547588,\n",
       "  0.7848761408083442,\n",
       "  0.7848761408083442,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7861799217731421,\n",
       "  0.7848761408083442,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7861799217731421,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7770534550195567,\n",
       "  0.7848761408083442,\n",
       "  0.7796610169491526,\n",
       "  0.7757496740547588,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7848761408083442,\n",
       "  0.788787483702738,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.78748370273794,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.78748370273794,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.788787483702738,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7757496740547588,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7757496740547588,\n",
       "  0.7835723598435462,\n",
       "  0.788787483702738,\n",
       "  0.7783572359843546,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7757496740547588,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.78748370273794,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7757496740547588,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.788787483702738,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7731421121251629,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7900912646675359,\n",
       "  0.7796610169491526,\n",
       "  0.7757496740547588,\n",
       "  0.7835723598435462,\n",
       "  0.7731421121251629,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7783572359843546,\n",
       "  0.7744458930899609,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7757496740547588,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7770534550195567,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7770534550195567,\n",
       "  0.7770534550195567,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.788787483702738,\n",
       "  0.788787483702738,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7861799217731421,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.78748370273794,\n",
       "  0.7835723598435462,\n",
       "  0.78748370273794,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7783572359843546,\n",
       "  0.7913950456323338,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7861799217731421,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7757496740547588,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7900912646675359,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7770534550195567,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7770534550195567,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7770534550195567,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.78748370273794,\n",
       "  0.7848761408083442,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7861799217731421,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7861799217731421,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.78748370273794,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7757496740547588,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7848761408083442,\n",
       "  0.7757496740547588,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7848761408083442,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7757496740547588,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.78748370273794,\n",
       "  0.7809647979139505,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7744458930899609,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7757496740547588,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.78748370273794,\n",
       "  0.7809647979139505,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7770534550195567,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7731421121251629,\n",
       "  0.7757496740547588,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7835723598435462,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7757496740547588,\n",
       "  0.7861799217731421,\n",
       "  0.7783572359843546,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7835723598435462,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7783572359843546,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7861799217731421,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7861799217731421,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7848761408083442,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.78748370273794,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7770534550195567,\n",
       "  0.7770534550195567,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7822685788787483,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7835723598435462,\n",
       "  0.7835723598435462,\n",
       "  0.7796610169491526,\n",
       "  0.7783572359843546,\n",
       "  0.7783572359843546,\n",
       "  0.7835723598435462,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7809647979139505,\n",
       "  0.7796610169491526,\n",
       "  0.7770534550195567,\n",
       "  0.7848761408083442,\n",
       "  0.7809647979139505,\n",
       "  0.78748370273794,\n",
       "  0.7783572359843546,\n",
       "  0.7822685788787483,\n",
       "  0.7835723598435462,\n",
       "  0.7770534550195567,\n",
       "  0.7770534550195567,\n",
       "  0.7796610169491526,\n",
       "  0.7822685788787483,\n",
       "  0.7822685788787483,\n",
       "  0.7796610169491526,\n",
       "  0.7809647979139505],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score_LR(loo,data,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Stratified K-Fold\n",
    "\n",
    "The Stratified K-Fold method is simply a version of K-Fold where each split has identical proportions of the target variable. But before trying this split, it's necessary to adapt our scoring function first.\n",
    "\n",
    "<b>21. Run the cell below to create a function named `avg_score_LR_skf` that will return the average score value for the train and the test set. This will have as parameters the partition technique you are going to use, your dependent variable and your independent variables. But, unlike the `avg_score_LR` function, it supports the use of `StratifiedKFold` to split the data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score_LR_skf(method,X,y):\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    for train_index, test_index in method.split(X,y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model = run_model_LR(X_train, y_train)\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_test = evaluate_model(X_test,y_test, model)\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Test:', np.mean(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>22. Import `StratifiedKFold` from `sklearn.model_selection`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>23. Create a `StratifiedKFold` instance and store it in `skf`. Then, call the function `avg_score_LR_skf` and check the average score for the train and the test sets using `skf`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7795136478087383\n",
      "Test: 0.7734791524265209\n"
     ]
    }
   ],
   "source": [
    "avg_score_LR_skf(skf,data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(X = X_train,y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7934782608695652"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597402597402597"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base:\n",
    "\n",
    "0.7869565217391304<br>\n",
    "0.7597402597402597\n",
    "\n",
    "k-fold:\n",
    "\n",
    "0.7832744284483407<br>\n",
    "0.7708304853041694\n",
    "\n",
    "repeated k-fold:\n",
    "\n",
    "0.7833333333333333<br>\n",
    "0.7747395833333334\n",
    "\n",
    "leave one out:\n",
    "\n",
    "0.7817168486527598<br>\n",
    "0.7786458333333334\n",
    "\n",
    "stratified k-fold:\n",
    "\n",
    "0.779368930008449<br>\n",
    "0.7734791524265209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SkLearn you have several options to select your model, and the application is similar to the cases we saw previously.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n",
    "\n",
    "\n",
    "### That's all for today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Image links (click this)</b></summary>\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\n",
    "    \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note:\n",
    "\n",
    "Please remember that just because the scores are better or worse when using **k-fold cross-validation** compared to the **hold-out method** does **not** mean that the model is inherently *\"better with k-fold.\"* These are two different techniques to evaluate the **same model**.\n",
    "\n",
    "- **K-fold cross-validation** is more robust because it evaluates the model across multiple subsets of the data, but it doesn't change the underlying model.\n",
    "- **Hold-out validation** evaluates the model on a single split of the data, which can lead to slightly different performance due to random variations in the split.\n",
    "\n",
    "In both cases, the model being evaluated is exactly the same. What differs is how we assess its performance. The models performance on unseen data will be **consistent**, regardless of the validation method used. These validation techniques help you **choose the best model** and evaluate its stability, but they do not change the models performance on new, unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should I do?\n",
    "\n",
    "The choice between using hold-out and k-fold cross-validation (or another cross-validation technique) to evaluate your model's performance depends on various factors, and each approach has its own advantages and disadvantages. The appropriate choice depends on your needs and project objectives.\n",
    "\n",
    "**1.  Hold-Out Method (`train_test_split`):**\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Faster: Simple data splitting is faster than k-fold cross-validation, which is useful when you have time or resource constraints.\n",
    "- Simplicity: It is easy to implement and understand, making it a good choice for quick analyses or prototyping.\n",
    "- Useful for initial assessments: It can be used for an initial assessment of the model before investing more time in cross-validation techniques.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Can be biased: Depending on how the data is split, results can be biased, as performance can vary significantly based on the random choice of training and testing data.\n",
    "- Does not capture variability: It does not take into account the variability of results due to different data splits, which can result in an optimistic or pessimistic evaluation of the model.\n",
    "\n",
    "**2. K-Fold Cross-Validation:**\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Robust evaluation: Provides a more robust and reliable evaluation of the model's performance, as it considers multiple data splits.\n",
    "- Better use of data: Utilizes the entire dataset for both training and testing in multiple iterations, reducing data wastage.\n",
    "- Helps detect overfitting: Allows you to detect whether the model is overfitting the training data.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- More time-consuming: It can be more time-consuming, especially with a large number of folds.\n",
    "- Complex to set up: Requires more setup and implementation than simple data splitting.\n",
    "- Not suitable for all cases: In some situations, such as when the data is highly imbalanced or when there are time constraints, using k-fold may not be ideal.\n",
    "\n",
    "In many cases, k-fold cross-validation is the default preference, but the choice depends on the specific circumstances of the project. You may also consider other cross-validation techniques, such as stratified cross-validation or leave-one-out cross-validation, depending on the project's requirements.\n",
    "\n",
    "### That's all!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c2e85b6140a684ef1c8cba9f0b90c35f8070bf96af0ec6708f210ef64ba8f2e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
