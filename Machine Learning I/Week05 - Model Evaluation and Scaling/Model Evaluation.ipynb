{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "In order to properly determine whether your model is good, it's important to use the appropriate <b>performance measures</b>. Naturally, the measures used in regression problems are different from the ones used in classification problems:\n",
    "\n",
    "* [1 - Regression Problems](#regression)\n",
    "* [2 - Classification Problems](#classification)\n",
    "\n",
    "But first, let's import pandas, train_test_split, the Linear Regression model from sklearn, and the metrics to be used in regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Regression problem model and metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, \\\n",
    "    mean_squared_error, median_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"regression\">\n",
    "\n",
    "## 1. Regression Problems\n",
    "</a>\n",
    "\n",
    "The objective in regression problems is to predict a continuous feature's values, and the metrics used to measure a model's performance are based on gauging the average difference between the real values and the predicted values, in multiple differnt ways. The measures discussed in this notebook are the following:\n",
    "\n",
    "* $R^{2}$ Score\n",
    "* Adjusted $R^{2}$ Score\n",
    "* MAE\n",
    "* RMSE\n",
    "* MedAE\n",
    " \n",
    "<b>1. Import the data located in 'Datasets/insurance.csv' with pandas and store it in a variable.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = pd.read_csv('Datasets/insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>female</th>\n",
       "      <th>northeast</th>\n",
       "      <th>northwest</th>\n",
       "      <th>southeast</th>\n",
       "      <th>smoker</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.884924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.725552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.449462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.984471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.866855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     bmi  children  female  northeast  northwest  southeast  smoker  \\\n",
       "0   19  27.900         0       1          0          0          0       1   \n",
       "1   18  33.770         1       0          0          0          1       0   \n",
       "2   28  33.000         3       0          0          0          1       0   \n",
       "3   33  22.705         0       0          0          1          0       0   \n",
       "4   32  28.880         0       0          0          1          0       0   \n",
       "\n",
       "     charges  \n",
       "0  16.884924  \n",
       "1   1.725552  \n",
       "2   4.449462  \n",
       "3  21.984471  \n",
       "4   3.866855  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Store the independent variables in a variable called `data`, and store the dependent variable (last column) in a variable called `target`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = insurance.iloc[:,:-1]\n",
    "target = insurance.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. By using the method train_test_split from sklearn.model_selection, split your dataset into a training set and a validation set, with the training set having 80% of the data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=15, \n",
    "                                                    shuffle=True, \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Create an instance of LinearRegression named `lr` with the default parameters and fit to the training dataset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Assign the predictions to `y_pred`, using the method `.predict()`.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.89021746, 25.23316771,  3.7913586 ,  3.2364895 ,  2.7537859 ,\n",
       "        8.03319434,  0.9868359 , 34.99635894,  8.44293274,  8.82010937,\n",
       "        3.89551369,  6.21106335, 35.977644  , 32.76835405,  5.51916539,\n",
       "       37.35753637, 27.19740132,  9.38117556, 30.15399798,  8.23776932,\n",
       "        5.44336484,  9.75778818,  3.38066877, 18.56202604, 11.84739888,\n",
       "        8.69127558,  7.68321386, 39.06950138,  3.7146851 , -0.04833778,\n",
       "        6.88226407,  9.27097592,  5.48424601, 41.11911931,  6.97111106,\n",
       "        5.57771534,  5.45822851,  3.82722533,  6.3289568 , 11.56151731,\n",
       "        7.1672574 , 10.78062088, 15.75070435,  2.72546684, 11.14961901,\n",
       "       11.23945479, 11.66962825,  6.45120447,  9.67410638, 28.65483353,\n",
       "        1.28648507,  2.15791909,  9.00472208, 10.26646054, 12.9356007 ,\n",
       "       26.76668774,  1.04292582, 12.36693775,  4.9768755 , 15.60985497,\n",
       "        6.43266019, 30.69092701, 25.07199352,  5.49482916, 34.37535533,\n",
       "       36.9069539 ,  4.98718735, 11.00109822, 11.49822537, 33.64899757,\n",
       "       10.69695161,  5.15917983, 10.88281205, 12.25282698, -0.07440306,\n",
       "        7.52319616, 37.07664734, 13.51757382, 19.19949024,  9.83103925,\n",
       "       12.73330098, 11.87251939, 39.25448139,  2.36007422,  7.89791466,\n",
       "       29.90390219, 16.09198391,  5.08433935, 11.79620847,  3.86159496,\n",
       "        5.65842675,  7.11061677,  4.02672404, 10.15008427,  9.87212553,\n",
       "        7.8802464 ,  9.68562924, 17.14929947, 13.11452259, 12.34908199,\n",
       "       11.85043645,  3.31370554, 10.81208451,  7.16352251, 14.35622191,\n",
       "       11.79632832, 15.49714039, 32.11051   , 11.14082071,  2.91593731,\n",
       "       26.85594485, 27.45002119,  3.80207654, 11.81515811,  6.18974137,\n",
       "        5.4320089 ,  4.21901716,  3.37279965,  4.27578156, 35.42314362,\n",
       "       11.09616251, 12.61950773,  3.71472624,  8.70845649, 10.68225164,\n",
       "       29.20390694,  4.32676786, 10.60906067, 35.18989533, 10.89025082,\n",
       "       11.7666998 ,  5.78923714, 12.82529957, 13.35790546, 14.7573166 ,\n",
       "       10.17322089, -0.23006247, 15.15070473,  6.51027989,  7.93687928,\n",
       "       34.09341513, 10.48835871,  5.12930649,  2.1241282 , 11.29625347,\n",
       "        9.08084056,  7.87575815, 10.05000501, 17.02567276,  6.94226822,\n",
       "        9.44052649,  5.43239087, 39.61879416,  9.50619191, 13.49741347,\n",
       "       11.73678374,  8.23887163, 10.25775182, 12.71898261,  0.58157601,\n",
       "       12.89128227, 10.87458228,  4.64353438,  6.93476189, 32.0722111 ,\n",
       "       11.24658748,  4.11646006,  5.6054325 , 33.48661663, 27.51647835,\n",
       "        9.55799224, 14.5474832 , 30.66611819, 27.3335083 , -1.55704554,\n",
       "        9.38752026, 28.793763  , 10.70127239, 34.01733667, 11.35646148,\n",
       "       35.36504685, 13.46546159,  7.74303926, 14.7529035 ,  0.33837546,\n",
       "        5.09675212,  6.97488463, 32.19149337,  8.87203579,  4.49629108,\n",
       "       16.01883163, 10.06520474, 10.35816117,  3.69590929, 10.31851582,\n",
       "       34.68871085,  4.24561523, 25.09736299,  9.45549561,  7.18697846,\n",
       "        5.82649614,  3.11642805, 14.44397686, 36.89099474, 10.55933766,\n",
       "        6.96849098,  9.62376901, 31.55424073, 12.1479277 ,  5.80876022,\n",
       "        8.94041492,  3.60498776,  8.32228211, 33.97624547,  7.14659297,\n",
       "       11.12094259,  7.44371615,  0.17055342, 11.13065255,  2.22945468,\n",
       "        9.25059897, 13.38287194,  8.55176003,  3.9821076 , 11.53395958,\n",
       "        1.65155415,  6.5916132 , 35.61023223, 30.84747055, 27.77449829,\n",
       "        8.84281974,  7.65060148,  7.69136067,  1.6599225 ,  0.72878352,\n",
       "        1.25423145,  4.00917091, 32.09205773, 23.70130215, -1.93908119,\n",
       "       27.15243942, 24.07838987,  5.82723215,  5.33135503, 13.03093566,\n",
       "        2.13199732,  0.30612184,  4.87655895, 15.28862519,  9.12593248,\n",
       "        9.53180653, 17.38860258,  2.00570851, 11.46897342, 26.34700076,\n",
       "        6.36360589, 30.5587124 , 11.68239992,  4.01703437,  2.25860166,\n",
       "        5.76005212, 38.85900286, 14.12568026,  7.86314473,  6.42244863,\n",
       "       15.29004721,  1.37595026, 12.91095817])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_val)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. $R^{2}$ Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score'>sklearn.metrics.r2_score(y_true, y_pred, ... )</a>\n",
    "\n",
    "The $R^{2}$ score, also known as the coefficient of determination, is used to determine how much variance in the target feature can be explained by the given model. Its value corresponds to the proportion of the target feature's variability that is explained by either the model or he independent variables. Its formula is the following:<br>\n",
    "\n",
    "$$R^{2} = 1-\\frac{\\sum_i{(y_{i}-\\hat{y_{i}})^{2}}}{\\sum_i{(y_{i}-\\bar{y})^{2}}} = 1-\\frac{SS_{Error}}{SS_{Total}}$$\n",
    "\n",
    "* $y_{i}$ - a real value\n",
    "* $\\hat{y_{i}}$ - a predicted value\n",
    "* $\\bar{y}$ - average of all real values<br>\n",
    "\n",
    "\n",
    "$R^{2}$'s highest possible value is 1 - when $R^{2}$ = 1, that means the entirety of the target feature's variance is accounted for. It can also have negative values - the model may be arbitrarily worse. <br>\n",
    "In Python, to calculate the $R^{2}$ we call the following <i>sklearn</i> function: `r2_score(real values,predictions)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Check the R^2 score of the model you created previously.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7709928565663494"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2. Adjusted $R^{2}$ Score\n",
    "\n",
    "\n",
    "There is no direct way to obtain the adjusted R^2 using sklearn, but we can apply the formula:\n",
    "\n",
    "$$\\bar{R^{2}} = 1-(1-R^{2})*\\frac{n-1}{n-p-1}$$\n",
    "with \n",
    "\n",
    "* $n$ - amount of observations\n",
    "* $p$ - amount of features <br>\n",
    "\n",
    "The adjusted $R^{2}$ score is a better option when we want to measure the amount of variance in the target variable that can be explained by our model. <b><i>But why?</b></i> <br>\n",
    "If extra features are added to out data, the regular $R^{2}$ score may increase, but it doesn't decrease, even if the new features are redundant. The adjusted $R^{2}$ score accounts for this since the number of features in the model is used in its calculation. Therefore, it's a more accurate measure of the proportion of variance of the dependent variable that is accounted for by the model. <br>\n",
    "\n",
    "<b>7. Calculate the Adjusted R^2 Score for your model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763919276846391\n"
     ]
    }
   ],
   "source": [
    "#single row\n",
    "print(1 - ((1 - r2_score(y_val, y_pred))*(len(y_val) - 1))/(len(y_val) - len(X_train.columns) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763919276846391"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a function:\n",
    "r2 = r2_score(y_val,y_pred)\n",
    "n = len(y_val)\n",
    "p = len(X_train.columns)\n",
    "\n",
    "def adjr2(r2,n,p):\n",
    "    return 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "adjr2(r2,n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "However in some cases we are more interested in quantifying the error in the same measuring unit of the variable - we can use metrics like MAE, MSE and MedAE for that. These metrics are't appropriate to tell how well the independent features may explain the dependent feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3. MAE (Mean absolute error)\n",
    "\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error'>sklearn.metrics.mean_absolute_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "The MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.<br>\n",
    "The MAE is always non-negative. The lower its value, the better the model.<br>\n",
    "In Python, to calculate the $R^{2}$ we call the following <i>sklearn</i> function: `mean_absolute_error(real values,predictions)`\n",
    "\n",
    "$$MAE = \\frac{1}{n}*\\sum_i{| y_{i}-\\hat{y_{i}}|}$$\n",
    "\n",
    "<b>8. Check the MAE of the model you created previously.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9285478094354755"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "### 1.4. RMSE (Root Mean squared error)\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error'>sklearn.metrics.mean_squared_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "The RMSE, like the MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.<br>\n",
    "It's also always non-negative. The lower its value, the better the model.<br>\n",
    "In Python, to calculate the RMSE we call the following <i>sklearn</i> function: `mean_squared_error(real values,predictions)`<br><br>\n",
    "In situations where **large errors** are particularly undesirable, it's preferable to use the RMSE over the MAE, as the RMSE punishes those kinds of errors. If the magnitude of the errors isn't very relevant, then the MAE is the better choice due to it being easier to interpret.<br>\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}*\\sum_i{( y_{i}-\\hat{y_{i}})^{2}}}$$\n",
    "\n",
    "<b>9. Check the RMSE of the model you created previously </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4641281603458545"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5. MedAE (Median absolute error)\n",
    "\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.median_absolute_error'>sklearn.metrics.median_absolute_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "Like the last two metrics, the MedAE measures the average magnitude of the errors in a set of predictions, without considering their direction.<br>\n",
    "It's always non-negative. The lower its value, the better the model.<br>\n",
    "In Python, to calculate the $R^{2}$ we call the following <i>sklearn</i> function: `median_absolute_error(real values,predictions)`<br><br>\n",
    "This metric is used when outliers are to be ignored in the assessment of the model's performance.<br>\n",
    "\n",
    "$$MedAE = median(| y_{i}-\\hat{y_{i}}|)$$\n",
    "\n",
    "<b>10. Check the MedAE score of the model you created previously.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4817072426177704"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"classification\">\n",
    "\n",
    "## 2. Classification Problems\n",
    "</a>\n",
    "\n",
    "The objective in classification problems is to predict the discrete values of a non-continuous feature, and the metrics used to measure a model's performance are based on determining the amount of (in)correctly classified results. The measures discussed in this notebook are the following:\n",
    "\n",
    "* Confusion matrix\n",
    "* Accuracy Score\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-Score\n",
    "* Classification report (includes most of the above)\n",
    "\n",
    "Before going further, let's review and take note of a few concepts:\n",
    "\n",
    "* <b>True Positive(TP)</b> - observation correctly classified as positive\n",
    "* <b>True Negative(TN)</b> - observation correctly classified as negative \n",
    "* <b>False Positive(FP)</b> - observation incorrectly classified as positive\n",
    "* <b>False Negative(FN)</b> - observation incorrectly classified as negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Import the Logistic Regression base model, and the metrics to be used for classification problems.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification problem model and metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, \\\n",
    "    precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Import the dataset from 'Datasets/winequality.csv', and define the independent variables as `data_w` and the dependent variable ('quality') as `target_w`. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "winequality = pd.read_csv('Datasets/winequality.csv')\n",
    "data_w = winequality.drop(['quality'], axis=1)\n",
    "target_w = winequality['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. By using the method train_test_split from sklearn.model_selection, split your dataset into a training set and a validation set, with the training set having 80% of the data. Set the random state to 15, and add `stratify = target_w`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data_w, \n",
    "                                                  target_w, \n",
    "                                                  train_size = 0.8, \n",
    "                                                  random_state=15, \n",
    "                                                  stratify = target_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Create an instance of LogisticRegression named as `log_model` with the default parameters, and fit it to your training data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Now that you have your model created, assign the predictions to `y_pred`, using the method `.predict()`.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_model.predict(X_val)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. The confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix'>sklearn.metrics.confusion_matrix(y_true, y_pred, ...)</a>\n",
    "\n",
    "The confusion matrix is a matrix composed by the amounts of TP, TN, FP, and FN, positioned like this:\n",
    "\n",
    "$$\\begin{bmatrix}TN & FP\\\\\n",
    "FN & TP\n",
    "\\end{bmatrix}^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Obtain the confusion matrix</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[576,  30],\n",
       "       [118,  60]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. The accuracy score\n",
    "\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score'>sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True,...)</a>\n",
    "\n",
    "The accuracy score is the proportion of correct predictions from the model to the total amount of observations.  <br>\n",
    "In Python, to calculate the accuracy we call the following <i>sklearn</i> function: `accuracy_score(real values,predictions, normalize = True.)`. If normalize is True, then the best performance is 1. When normalize is False, then the best performance is the number of samples. Either way, the higher the value, the better.<br>\n",
    "The accuracy is generally a good performance measure, but it's inadequate in situations where the cost of certain kinds of errors is too high:\n",
    "* Detection of viral illnesses: The objective is detecting positive cases, the price of false negatives is too high, since said cases can spread the illness when they go undetected; In this cases the <i>recall</i> metric is very useful\n",
    "* Detection of spam e-mails: E-mails incorrectly flagged as spam by the model (false positives) won't show up, and important information may potentially be lost; In this cases the <i>precision</i> metric is very useful\n",
    "\n",
    "$$accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7. Get the accuracy score</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112244897959183"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### 2.3. The precision\n",
    "\n",
    "\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score'>sklearn.metrics.precision_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "The precision score is the proportion of correct positive predictions from the model to the total amount of positive predictions.  <br>\n",
    "In Python, to calculate the precision we call the following <i>sklearn</i> function: `precision_score(real values,predictions)`.<br>\n",
    "This metric is particularly good in the assessment of models where false positives are particularly dangerous, since those are one of the only two variables accounted for by the metric.\n",
    "\n",
    "$$recall = \\frac{TP}{TP+FP}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>8. Get the precision score</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4. The recall\n",
    "\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.recall_score'>sklearn.metrics.recall_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "The recall score is the proportion of correct positive predictions from the model to the total amount of observations that are positive (true positives + false negatives).  <br>\n",
    "In Python, to calculate the precision we call the following <i>sklearn</i> function: `recall_score(real values,predictions)`.<br>\n",
    "This metric is particularly good in the assessment of models where false negatives are particularly dangerous, since those are one of the only two variables accounted for by the metric.\n",
    "\n",
    "$$recall = \\frac{TP}{TP+FN}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>9. Get the recall score</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34831460674157305"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.5. The F1 Score\n",
    "\n",
    "\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score'>sklearn.metrics.f1_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "The F1-score is the most balanced of the common performance measures for classification problems.  <br>\n",
    "It's useful in two distinct, but not mutually exclusive situations:\n",
    "* When you seek a balance petween precision and recall (i.e. when both false positives and false negatives are important)\n",
    "* When you have an uneven class distribution (a large number of negative cases)<br>\n",
    "\n",
    "It's obvious how the F1-score helps with the first situation. In the second situation, the F1-score is helpful because, unlike the accuracy, it doesn't involve observations that are correctly classified as negative (true negatives). True negatives often have little to no significance in classification problems, but greatly influence the accuracy. So in cases where there's an uneven class distribution with lots of True negatives, the F1-score is advised because, unlike the accuracy, it'snot influenced by the True negatives.<br><br>\n",
    "In Python, to calculate the precision we call the following <i>sklearn</i> function: `f1_score(real values,predictions)`.<br>\n",
    "\n",
    "$$F1 = 2*\\frac{precision*recall}{precision+recall}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>10. Get the F1 Score</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44776119402985076"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.6. The Classification report\n",
    "\n",
    "sklearn documentation: <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score'>sklearn.metrics.f1_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "The classification report shows the precision, recall, and F1-scores, rounded to 2 decimals, for our model. It shows these values for both possibilities: predicting the majority class and predicting the minority class. It also shows the overall accuracy.\n",
    "\n",
    "<b>11. _Print_ the classification report</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89       606\n",
      "           1       0.67      0.34      0.45       178\n",
      "\n",
      "    accuracy                           0.81       784\n",
      "   macro avg       0.75      0.64      0.67       784\n",
      "weighted avg       0.79      0.81      0.79       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Whole analysis\n",
    "\n",
    "Now, let's run a complete analysis, on training data and on validation data, using the classification report:\n",
    "\n",
    "<b>12. Run the cell below to create a function named `metrics` that will print the results of the classification report and the confusion matrix for both datasets (train and validation)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_train, pred_train , y_val, pred_val):\n",
    "    print('_'*107)\n",
    "    print('                                                     TRAIN                                                 \\n')\n",
    "    print(classification_report(y_train, pred_train))\n",
    "    print('Confusion matrix:\\n')\n",
    "    print(confusion_matrix(y_train, pred_train))\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    \n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                VALIDATION                                                 \\n')\n",
    "    print(classification_report(y_val, pred_val))\n",
    "    print('Confusion matrix:\\n')\n",
    "    print(confusion_matrix(y_val, pred_val))\n",
    "    print('___________________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>13. Create an object named ``labels_train`` that will containt the predicted values for the train and another one named ``labels_val`` that will contain the predicted values for the validation set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = log_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>14. Call the function `metrics()` defined previously, and define the arguments: <br> (`y_train = y_train`, `pred_train = y_pred_train` , `y_val = y_val`, `pred_val = y_pred`)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________________\n",
      "                                                     TRAIN                                                 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88      2424\n",
      "           1       0.61      0.29      0.40       712\n",
      "\n",
      "    accuracy                           0.80      3136\n",
      "   macro avg       0.72      0.62      0.64      3136\n",
      "weighted avg       0.77      0.80      0.77      3136\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[2292  132]\n",
      " [ 504  208]]\n",
      "___________________________________________________________________________________________________________\n",
      "___________________________________________________________________________________________________________\n",
      "                                                VALIDATION                                                 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89       606\n",
      "           1       0.67      0.34      0.45       178\n",
      "\n",
      "    accuracy                           0.81       784\n",
      "   macro avg       0.75      0.64      0.67       784\n",
      "weighted avg       0.79      0.81      0.79       784\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[576  30]\n",
      " [118  60]]\n",
      "___________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "metrics(y_train,y_pred_train,y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br>\n",
    "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d <br>\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### That's all!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c2e85b6140a684ef1c8cba9f0b90c35f8070bf96af0ec6708f210ef64ba8f2e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
